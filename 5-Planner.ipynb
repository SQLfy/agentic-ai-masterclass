{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ebd7963-64a3-4440-8934-5e451f6f191a",
   "metadata": {},
   "source": [
    "# Exercise 5: Planner pattern implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179dbd0-c430-4807-88e8-86a31cbe5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcfaf6-10f3-43da-a82b-bcb4626e9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, TypedDict, Union\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --- Setup LLM ---\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# --- 1. Define the State ---\n",
    "class PlanState(TypedDict):\n",
    "    request: str                # The user's original request\n",
    "    plan: List[str]             # The queue of remaining steps\n",
    "    past_steps: List[str]       # Steps we have already finished\n",
    "    results: List[str]          # The data collected from executed steps\n",
    "\n",
    "# --- 2. Define the PROMPTS ---\n",
    "\n",
    "PLANNER_PROMPT = \"\"\"\n",
    "You are a Project Manager. \n",
    "Break down the following user request into 3 distinct, actionable steps.\n",
    "Return ONLY the steps as a Python list of strings. Do not add numbering or extra text.\n",
    "\n",
    "Example Output:\n",
    "[\"Search for X\", \"Analyze Y\", \"Write Z\"]\n",
    "\n",
    "User Request: {request}\n",
    "\"\"\"\n",
    "\n",
    "EXECUTOR_PROMPT = \"\"\"\n",
    "You are a Specialist Worker.\n",
    "Execute the following step: \"{step}\"\n",
    "Context from previous steps: {context}\n",
    "\n",
    "(Note: Since I cannot browse the real internet, simulate the research by generating realistic, detailed information about this topic.)\n",
    "\"\"\"\n",
    "\n",
    "REPORTER_PROMPT = \"\"\"\n",
    "You are a Final Reporter.\n",
    "Read the following research results and compile a final answer to the original request.\n",
    "\n",
    "Original Request: {request}\n",
    "Results:\n",
    "{results}\n",
    "\"\"\"\n",
    "\n",
    "# --- 3. Define the Nodes ---\n",
    "\n",
    "def planner_node(state: PlanState):\n",
    "    print(f\"\\n--- PLANNER: Analyzing request '{state['request']}' ---\")\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=PLANNER_PROMPT.format(request=state['request']))\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Simple parsing to turn the string representation of a list into an actual list\n",
    "    # (In production, use structured outputs/Pydantic)\n",
    "    try:\n",
    "        # This is a hacky way to parse \"[...]\" string to list for this demo\n",
    "        import ast\n",
    "        plan = ast.literal_eval(response.content)\n",
    "    except:\n",
    "        # Fallback if LLM messes up format\n",
    "        plan = response.content.split(\"\\n\")\n",
    "\n",
    "    print(f\"--- PLANNER: Created roadmap: {plan} ---\")\n",
    "    \n",
    "    return {\"plan\": plan, \"results\": [], \"past_steps\": []}\n",
    "\n",
    "def executor_node(state: PlanState):\n",
    "    # Get the next step\n",
    "    plan = state[\"plan\"]\n",
    "    current_step = plan[0]\n",
    "    \n",
    "    print(f\"--- EXECUTOR: Working on step: '{current_step}' ---\")\n",
    "    \n",
    "    # context helps the LLM know what happened before (optional)\n",
    "    context = \"\\n\".join(state[\"results\"])\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=EXECUTOR_PROMPT.format(step=current_step, context=context))\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    result_text = f\"Step: {current_step}\\nResult: {response.content}\"\n",
    "    \n",
    "    # Update State: Remove current step from plan, add to past_steps, add result\n",
    "    return {\n",
    "        \"plan\": plan[1:], \n",
    "        \"past_steps\": [current_step],\n",
    "        \"results\": [result_text] # In LangGraph this usually appends if you use Annotated[List, operator.add]\n",
    "                                 # Here we are just returning the list to be merged by the graph\n",
    "    }\n",
    "\n",
    "def reporter_node(state: PlanState):\n",
    "    print(\"\\n--- REPORTER: Summarizing all findings ---\")\n",
    "    \n",
    "    # Combine all results into one string\n",
    "    all_results = \"\\n\\n\".join(state[\"results\"])\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=REPORTER_PROMPT.format(request=state['request'], results=all_results))\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"results\": [response.content]} # Overwrite or append final result\n",
    "\n",
    "# --- 4. Define the Logic (The Router) ---\n",
    "\n",
    "def should_continue(state: PlanState):\n",
    "    if len(state[\"plan\"]) > 0:\n",
    "        return \"executor\" # Loop back to do the next step\n",
    "    else:\n",
    "        return \"reporter\" # All steps done\n",
    "\n",
    "# --- 5. Build the Graph ---\n",
    "\n",
    "workflow = StateGraph(PlanState)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"executor\", executor_node)\n",
    "workflow.add_node(\"reporter\", reporter_node)\n",
    "\n",
    "# Add Edges\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"executor\")\n",
    "\n",
    "# The Conditional Loop\n",
    "workflow.add_conditional_edges(\n",
    "    \"executor\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"executor\": \"executor\",\n",
    "        \"reporter\": \"reporter\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"reporter\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- 6. Run it ---\n",
    "# Note: \"results\" uses a reducer logic in real apps, here we just simplified for clarity\n",
    "# If you run this, you will see the 'results' list might behave differently depending on graph config.\n",
    "# For this simple demo, we assume the state merges nicely.\n",
    "\n",
    "print(\">>> STARTING PLANNING AGENT\")\n",
    "inputs = {\n",
    "    \"request\": \"Research the history of the Eiffel Tower, focusing on its construction and initial reception.\",\n",
    "    \"plan\": [],\n",
    "    \"results\": [], \n",
    "    \"past_steps\": []\n",
    "}\n",
    "\n",
    "# The invoke will run the loop until plan is empty\n",
    "final_output = app.invoke(inputs)\n",
    "\n",
    "print(\"\\n\\n>>> FINAL REPORT <<<\")\n",
    "# The last item in results is the reporter's summary\n",
    "print(final_output[\"results\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61acbcd7-fa91-42ff-817d-3ab4f2f6cb55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
